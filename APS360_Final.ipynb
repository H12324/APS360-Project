{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360 Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H12324/APS360-Project/blob/main/APS360_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgCOtOU9Cd4Y"
      },
      "outputs": [],
      "source": [
        "from skimage import io, color\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "\n",
        "\n",
        "!pip install timm #for CSPDarknet52 pretrained\n",
        "import timm\n",
        "\n",
        "!pip install torchinfo #for generating model summaries\n",
        "from torchinfo import summary\n",
        "\n",
        "!pip install piqa #for psnr/ssim\n",
        "import piqa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation"
      ],
      "metadata": {
        "id": "MwlCUQegxm3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT SAVED AS NPARRAY!! (because all image objects in all libraries are RGB afaik)\n",
        "\n",
        "# input: folder of RGB images\n",
        "# output: folder of LAB image, with padding and cropped \n",
        "\n",
        "def preprocess_images(input_path, output_path): \n",
        "  if not os.path.exists(output_path): \n",
        "    os.mkdir(output_path)\n",
        "  with os.scandir(input_path) as entries:\n",
        "    count =0\n",
        "    for entry in entries:\n",
        "        path_name = input_path+ '/'+ entry.name\n",
        "      #  print(path_name)\n",
        "    #    rgb = io.imread(path_name)\n",
        "        rgb = Image.open(path_name)\n",
        "        rgbplot = plt.imshow(rgb)\n",
        "        plt.show()\n",
        "\n",
        "        desired_size = 640\n",
        "\n",
        "        old_size = rgb.size  # old_size is in (width, height) format\n",
        "        print(old_size)\n",
        "        delta_w = desired_size - old_size[0]\n",
        "        delta_h = desired_size - old_size[1]\n",
        "        padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "        rgb = ImageOps.expand(rgb, padding) # padding done\n",
        "\n",
        "        rgb_resized = rgb.resize((256,256)) #resizing\n",
        "        resized = plt.imshow(rgb_resized)\n",
        "        plt.show()\n",
        "        \n",
        "        rgb_resized=np.asarray(rgb_resized)\n",
        "        rgb_resized = rgb_resized[:, :, :3]\n",
        "    #    print(rgb_resized.shape)\n",
        "        lab = color.rgb2lab(rgb_resized)\n",
        "\n",
        "   #     print(lab.shape)\n",
        "        rgbAgain = color.lab2rgb(lab)\n",
        "        rgbAgainplot = plt.imshow(rgbAgain)\n",
        "        plt.show()\n",
        "\n",
        "        labcopy = lab\n",
        "        labcopy[:, :, 1] = 0\n",
        "        labcopy[:, :, 2] = 0\n",
        "        grey = color.lab2rgb(labcopy)\n",
        "        greyplot = plt.imshow(grey)\n",
        "        plt.show()\n",
        "\n",
        "      #  img = Image.fromarray(lab)\n",
        "        np.save(output_path+'/'+entry.name, lab)\n"
      ],
      "metadata": {
        "id": "Bw6tVLTB4zMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For creating COCO based dataset"
      ],
      "metadata": {
        "id": "YIShmBoEFYDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "!unzip \"/content/annotations_trainval2017.zip\""
      ],
      "metadata": {
        "id": "bRkjA3rjCjrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from https://stackoverflow.com/questions/51100191/how-can-i-download-a-specific-part-of-coco-dataset\n",
        "from pycocotools.coco import COCO\n",
        "import requests\n",
        "# instantiate COCO specifying the annotations json path\n",
        "coco = COCO('/content/annotations/instances_train2017.json')"
      ],
      "metadata": {
        "id": "kR7ZzuMOJVJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mkdir coco_person_raw"
      ],
      "metadata": {
        "id": "0z4NJMz_o3RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm coco_person_raw -rf "
      ],
      "metadata": {
        "id": "A07RUMF_Lf81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catIds = coco.getCatIds(catNms=['person']) # we focus on people\n",
        "\n",
        "imgIds = coco.getImgIds(catIds=catIds)\n",
        "print (imgIds)"
      ],
      "metadata": {
        "id": "NkL0lBu2jkzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 1600 # number to download\n",
        "\n",
        "# Specify a list of category names of interest\n",
        "catIds = coco.getCatIds(catNms=['person']) # we focus on people\n",
        "# Get the corresponding image ids and images using loadImgs\n",
        "imgIds = coco.getImgIds(catIds=catIds)\n",
        "images = coco.loadImgs(imgIds)\n",
        "print(images[:1]) #to show how it works\n",
        "# Save the images into a local folder\n",
        "i=1\n",
        "for im in random.sample(images, samples):\n",
        "    img_data = requests.get(im['coco_url']).content\n",
        "    with open('/content/coco_person_raw/' + im['file_name'], 'wb') as handler:\n",
        "       handler.write(img_data)\n",
        "       print(i)\n",
        "       i+=1"
      ],
      "metadata": {
        "id": "AOODY7FHKQLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create dataset folders (train/val/test)\n",
        "(also does resizing + padding if needed and not already preprocessed)"
      ],
      "metadata": {
        "id": "13FyGDB8FtWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exports dataset in lab format with padding and resizing\n",
        "from skimage import color\n",
        "\n",
        "# preprocess_images(\"/content/landscapes_dataset/archive\",\"/content/landscape_preprocessed\")\n",
        "random.seed(100)\n",
        "folders = ['train/', 'test/','val/']\n",
        "\n",
        "#source = '/content/coco_person_raw/'\n",
        "#dest_folders = ['/content/small_dataset/' + i for i in folders] \n",
        "\n",
        "source = '/content/landscapes_dataset/archive'\n",
        "dest_folders = ['/content/large_dataset/' + i for i in folders]\n",
        "\n",
        "samples = 4320 #total images to be split, must be divisible by 32\n",
        "\n",
        "files =  os.listdir(source)\n",
        "train_imgs = files[:int(0.8*samples)]\n",
        "val_imgs = files[int(0.8*samples):int(0.9*samples)]\n",
        "test_imgs = files[int(0.9*samples):]\n",
        "print (len(train_imgs), len(val_imgs), len(test_imgs))\n",
        "\n",
        "small_size = 128 #resize to 128 x 128\n",
        "\n",
        "print(test_imgs)\n",
        "for i in dest_folders:\n",
        "  os.makedirs(i, exist_ok= True)\n",
        "\n",
        "for img in train_imgs: \n",
        "  rgb = Image.open(os.path.join(source,img))\n",
        "  old_size = rgb.size  # old_size is in (width, height) format\n",
        "  desired_size = max(old_size)\n",
        "  delta_w = desired_size - old_size[0]\n",
        "  delta_h = desired_size - old_size[1]\n",
        "  padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "  new_im = ImageOps.expand(rgb, padding) # padding done\n",
        "  new_im = new_im.resize((small_size,small_size)) # resizing done\n",
        "  # https://stackoverflow.com/questions/3228361/using-pil-and-numpy-to-convert-an-image-to-lab-array-modify-the-values-and-then\n",
        "  npArray = np.asarray(new_im)\n",
        "  if npArray.shape[-1] == 3:\n",
        "    labArray = color.rgb2lab(npArray) # Convert array from RGB into Lab\n",
        "    np.save(os.path.join(dest_folders[0],img.split('.')[0]),labArray) # saving\n",
        "  else:\n",
        "    print (f\"{img.split('.')[0]} not used\")  \n",
        " \n",
        "\n",
        "\n",
        "for img in val_imgs: \n",
        "  rgb = Image.open(os.path.join(source,img))\n",
        "  old_size = rgb.size  # old_size is in (width, height) format\n",
        "  delta_w = desired_size - old_size[0]\n",
        "  delta_h = desired_size - old_size[1]\n",
        "  padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "  new_im = ImageOps.expand(rgb, padding) # padding done\n",
        "  new_im = new_im.resize((small_size,small_size)) # resizing done\n",
        "  # https://stackoverflow.com/questions/3228361/using-pil-and-numpy-to-convert-an-image-to-lab-array-modify-the-values-and-then\n",
        "  npArray = np.asarray(new_im)\n",
        "  labArray = color.rgb2lab(npArray) # Convert array from RGB into Lab\n",
        "  np.save(os.path.join(dest_folders[2],img.split('.')[0]),labArray) # saving\n",
        "  \n",
        "for img in test_imgs: \n",
        "  rgb = Image.open(os.path.join(source,img))\n",
        "  old_size = rgb.size  # old_size is in (width, height) format\n",
        "  delta_w = desired_size - old_size[0]\n",
        "  delta_h = desired_size - old_size[1]\n",
        "  padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "  new_im = ImageOps.expand(rgb, padding) # padding done\n",
        "  new_im = new_im.resize((small_size,small_size)) # resizing done\n",
        "  # https://stackoverflow.com/questions/3228361/using-pil-and-numpy-to-convert-an-image-to-lab-array-modify-the-values-and-then\n",
        "  npArray = np.asarray(new_im)\n",
        "  labArray = color.rgb2lab(npArray) # Convert array from RGB into Lab\n",
        "  np.save(os.path.join(dest_folders[1],img.split('.')[0]),labArray) # saving\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "x-v57DzXMFC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Helper Functions"
      ],
      "metadata": {
        "id": "GkqOTDGKGydm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function to save single image with \n",
        "\n",
        "rgb = Image.open(\"/content/landscapes_dataset/archive/00000000.jpg\")\n",
        "old_size = rgb.size  # old_size is in (width, height) format\n",
        "delta_w = desired_size - old_size[0]\n",
        "delta_h = desired_size - old_size[1]\n",
        "padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "new_im = ImageOps.expand(rgb, padding) # padding done\n",
        "new_im = new_im.resize((small_size,small_size)) # resizing done\n",
        "# https://stackoverflow.com/questions/3228361/using-pil-and-numpy-to-convert-an-image-to-lab-array-modify-the-values-and-then\n",
        "npArray = np.asarray(new_im)\n",
        "labArray = color.rgb2lab(npArray) # Convert array from RGB into Lab\n",
        "np.save(os.path.join(dest_folders[0],img.split('.')[0]),labArray) # saving"
      ],
      "metadata": {
        "id": "gDaNMsyUAyFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can be used to convert lab to rgb effectively for single image\n",
        "img_array = np.load('/content/small_dataset/train/000000000673.npy')\n",
        "end = color.lab2rgb(img_array)*255\n",
        "end = end.astype(np.uint8)\n",
        "end = Image.fromarray(end, \"RGB\")\n",
        "end = plt.imshow(end)\n",
        "\n",
        "print(img_array.shape)"
      ],
      "metadata": {
        "id": "V7DOkpDGSTbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a copy of the small dataset so you dont have to repeat all of the above everytime\n",
        "!zip -r large_dataset.zip large_dataset\n",
        "from google.colab import files\n",
        "files.download(\"/content/large_dataset.zip\")"
      ],
      "metadata": {
        "id": "bQcIuQ-VaLfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5BVheCP_JJ2"
      },
      "outputs": [],
      "source": [
        "# DELETE DATASET\n",
        "!rm -r /content/small_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vdldK_Vefim"
      },
      "outputs": [],
      "source": [
        "############ Testing ####################\n",
        "train_loader, val_loader = split_data('/content/large_dataset', 16)\n",
        "\n",
        "print(train_loader)\n",
        "\n",
        "for inputs, labels in val_loader:\n",
        "  print('\\n')\n",
        "  print(inputs.shape)\n",
        "  print(labels.shape)\n",
        "  print('\\n')\n",
        "\n",
        "  to_rgb(inputs[0], labels[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgYUiw7HynoA"
      },
      "outputs": [],
      "source": [
        "### Needed omly when above fails ##########\n",
        "\n",
        "path = \"/content/landscapes_dataset/archive\"\n",
        "# Remove any partially unzipped file\n",
        "input_filenames = os.listdir(path)\n",
        "for filename in input_filenames :\n",
        "  if filename == \".ipynb_checkpoints\":\n",
        "    shutil.rmtree(os.path.join(path, filename))\n",
        "    continue\n",
        "  image = np.load(path + \"/\" + filename)\n",
        "  if image.shape[2] != 3:\n",
        "    print(\"Deleted image with shape: \" + str(image.shape))\n",
        "    os.remove(os.path.join(path, filename)) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading existing dataset\n"
      ],
      "metadata": {
        "id": "GjSzEimiH9FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if testing with a set dataset\n",
        "!unzip large_dataset.zip "
      ],
      "metadata": {
        "id": "BECrI5GJx0pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"/content/large_dataset/train\")))"
      ],
      "metadata": {
        "id": "J2o0FhoZKUGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec3f801-a6e8-43d5-ceac-1dc800bfc2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IF FOLDER HAS .IPYNB CHECKPOINTS, TRAINING WILL STOP\n",
        "!rm -r /content/large_dataset/train/.ipynb_checkpoints"
      ],
      "metadata": {
        "id": "lvQ5e4WCMGPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper fns"
      ],
      "metadata": {
        "id": "mGlBv25m8zW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uOGwRJSo89sR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fd071a-ee0a-4180-a754-ca1852a0db6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For quickly visualising a few images from training and validation set\n",
        "\n",
        "def quick_qualitative(model, dataset, samples):\n",
        "  t_loader, v_loader = split_data(dataset, 1)\n",
        "\n",
        "  numb_t = 0\n",
        "\n",
        "  for inputs, labels in t_loader:\n",
        "    numb_t += 1\n",
        "    print(\"Demonstrating Some Train Set Results\")\n",
        "    inputs = inputs.cuda()\n",
        "    labels = labels.cuda()\n",
        "    embed = cspdarknet53(inputs)[-1].to(device)\n",
        "\n",
        "    logits = model(inputs,embed)\n",
        "    print(\"Attempt at Reconstruction\")\n",
        "    to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "    print(\"Original\")\n",
        "    to_rgb(inputs.cpu()[0], labels.cpu()[0])\n",
        "    if (numb_t == samples):\n",
        "      break\n",
        "  numb_v = 0\n",
        "  for inputs, labels in v_loader:\n",
        "    numb_v += 1\n",
        "    print(\"Demonstrating Val Set Results\")\n",
        "    inputs = inputs.cuda()\n",
        "    labels = labels.cuda()\n",
        "    embed = cspdarknet53(inputs)[-1].to(device)\n",
        "\n",
        "    logits = model(inputs,embed)\n",
        "    print(\"Attempt at Reconstruction\")\n",
        "    to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "    print(\"Original\")\n",
        "    to_rgb(inputs.cpu()[0], labels.cpu()[0])\n",
        "    if (numb_v == samples):\n",
        "      break\n",
        "  "
      ],
      "metadata": {
        "id": "mdwOMIC7uLqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grey LAB to 3 channel RGB function, NOT USED\n",
        "def greyLAB2RGB(grayscale,batch_size): \n",
        "  #resultrgb = np.empty((batch_size, 3, 128, 128))\n",
        "  resultrgb = []\n",
        "  for i, bw in enumerate(grayscale):\n",
        "    bw_reshaped = bw.reshape(128,128, 1)\n",
        "    image_bwlab =  np.append(bw_reshaped, np.zeros((128, 128, 2)), axis = 2)\n",
        "    bwrgb = color.lab2rgb(image_bwlab)*255\n",
        "    # colour = color.lab2rgb(image_bwlab)*255\n",
        "    # end = colour.astype(np.uint8)\n",
        "    # end = Image.fromarray(end, \"RGB\")\n",
        "    # plt.imshow(end)\n",
        "    # plt.show()\n",
        "    bwrgb_reshaped = bwrgb.reshape(3, 128, 128)\n",
        "    # print(bwrgb_reshaped)\n",
        "    resultrgb.append(list(bwrgb_reshaped))\n",
        "    #np.append(resultrgb[i],bwrgb_reshaped)\n",
        "  # return torch.from_numpy(resultrgb).float()\n",
        "  # tensorver = torch.FloatTensor(resultrgb)\n",
        "  # print(tensorver.size())\n",
        "  return torch.FloatTensor(resultrgb)\n",
        "# train_loader, val_loader = split_data('/content/small_dataset', 16)\n",
        "# for inputs, labels in val_loader:\n",
        "#   bw_rgb = greyLAB2RGB(inputs,16)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xXBOWa-TK1A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwFNgofaEha2"
      },
      "outputs": [],
      "source": [
        "## Trainer Helper - for saving data\n",
        "def get_model_name(batch_size, learning_rate, epoch):\n",
        "    path = \"bs{}_lr{}_epoch{}\".format(batch_size, learning_rate, epoch)\n",
        "    return path\n",
        "\n",
        "class AverageMeter(object):\n",
        "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
        "  img_array = np.zeros((128, 128, 3))\n",
        "  img_array[:,:,0] = np.squeeze( np.swapaxes(grayscale_input, 0, 1) )\n",
        "  img_array[:,:,1:] = np.swapaxes( np.swapaxes(ab_input, 0, 2), 0, 1 )\n",
        "\n",
        "  colour = color.lab2rgb(img_array)*255\n",
        "  end = colour.astype(np.uint8)\n",
        "  end = Image.fromarray(end, \"RGB\")\n",
        "  plt.imshow(end)\n",
        "  plt.show()\n",
        "\n",
        "  # DEBUGGING: Showing BW Image\n",
        "  # presentable_bw = np.repeat( np.expand_dims(color.rgb2gray(colour/255.0), axis=2), 3, axis=2 )\n",
        "  # plt.imshow(presentable_bw)\n",
        "  # plt.show()\n",
        "\n",
        "  # TODO: Fix Saving the file\n",
        "  #if save_path is not None and save_name is not None: \n",
        "    #presentable_bw = np.repeat( np.expand_dims(color.rgb2gray(colour/255.0), axis=2), 3, axis=2 )\n",
        "    #plt.imsave(arr=presentable_bw, fname='{}{}'.format(save_path['grayscale'], save_name))\n",
        "    #plt.imsave(arr=end, fname='{}{}'.format(save_path['colorized'], save_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "BXgEJ4a3HZrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##models which were used for inspiration/in development\n",
        "\n"
      ],
      "metadata": {
        "id": "SI2jx9Sbv6Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UNET implementatino for inspiration, from https://towardsdatascience.com/u-net-b229b32b4a71\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def contracting_block(self, in_channels, out_channels, kernel_size=3):\n",
        "        block = torch.nn.Sequential(\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(out_channels),\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=out_channels, out_channels=out_channels),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(out_channels),\n",
        "                )\n",
        "        return block\n",
        "    \n",
        "    def expansive_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
        "            block = torch.nn.Sequential(\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channel),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(mid_channel),\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=mid_channel),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(mid_channel),\n",
        "                    torch.nn.ConvTranspose2d(in_channels=mid_channel, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "                    )\n",
        "            return  block\n",
        "    \n",
        "    def final_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
        "            block = torch.nn.Sequential(\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channel),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(mid_channel),\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=mid_channel),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(mid_channel),\n",
        "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=out_channels, padding=1),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.BatchNorm2d(out_channels),\n",
        "                    )\n",
        "            return  block\n",
        "    \n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UNet, self).__init__()\n",
        "        #Encode\n",
        "        self.conv_encode1 = self.contracting_block(in_channels=in_channel, out_channels=64)\n",
        "        self.conv_maxpool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv_encode2 = self.contracting_block(64, 128)\n",
        "        self.conv_maxpool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv_encode3 = self.contracting_block(128, 256)\n",
        "        self.conv_maxpool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        # Bottleneck\n",
        "        self.bottleneck = torch.nn.Sequential(\n",
        "                            torch.nn.Conv2d(kernel_size=3, in_channels=256, out_channels=512),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.BatchNorm2d(512),\n",
        "                            torch.nn.Conv2d(kernel_size=3, in_channels=512, out_channels=512),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.BatchNorm2d(512),\n",
        "                            torch.nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "                            )\n",
        "        # Decode\n",
        "        self.conv_decode3 = self.expansive_block(256, 256, 128)\n",
        "        self.conv_decode2 = self.expansive_block(128, 128, 64)\n",
        "        self.final_layer = self.final_block(64, 64, out_channel)\n",
        "        \n",
        "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
        "        if crop:\n",
        "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
        "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
        "        return torch.cat((upsampled, bypass), 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        encode_block1 = self.conv_encode1(x)\n",
        "        encode_pool1 = self.conv_maxpool1(encode_block1)\n",
        "        encode_block2 = self.conv_encode2(encode_pool1)\n",
        "        encode_pool2 = self.conv_maxpool2(encode_block2)\n",
        "        encode_block3 = self.conv_encode3(encode_pool2)\n",
        "        encode_pool3 = self.conv_maxpool3(encode_block3)\n",
        "        # Bottleneck\n",
        "        bottleneck1 = self.bottleneck(encode_pool3)\n",
        "        # Decode\n",
        "        decode_block3 = self.crop_and_concat(bottleneck1, encode_block3)\n",
        "        cat_layer2 = self.conv_decode3(decode_block3)\n",
        "        decode_block2 = self.crop_and_concat(cat_layer2, encode_block2)\n",
        "        cat_layer1 = self.conv_decode2(decode_block2)\n",
        "        decode_block1 = self.crop_and_concat(cat_layer1, encode_block1)\n",
        "        final_layer = self.final_layer(decode_block1)\n",
        "        return  final_layer"
      ],
      "metadata": {
        "id": "kl3KvAQi_TXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#our model with instance without unet res connections\n",
        "class colorizer(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.name = \"colorizer\"\n",
        "    super(colorizer, self).__init__()\n",
        "    self.Conv0 = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding = 'same'), # 128 * 128 * 32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.Conv1 = nn.Sequential(\n",
        "    nn.Conv2d(32, 64, 3, stride = 2, padding = 1), # 64*64*64\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv2 = nn.Sequential(\n",
        "    nn.Conv2d(64, 128, 2, padding = 'same'), # 64 * 64 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.Conv3 = nn.Sequential(\n",
        "    nn.Conv2d(128, 256, 2, stride = 2, padding = 0), # 32 * 32 * 256\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv4 = nn.Sequential(\n",
        "    nn.Conv2d(256, 512, 2,stride = 2, padding = 0), # 32 * 32 * 512\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.fused = nn.Sequential(\n",
        "    nn.Conv2d(1024, 512, 2, padding = 'same'), # 32 * 32 * 512\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv5 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv6 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    \"\"\"self.Conv6 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(256, 128, 2, output_padding = 32), # 64 * 64 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\"\"\"\n",
        "    self.Conv7 = nn.Sequential(\n",
        "    nn.Conv2d(128, 64, 2, padding = 'same'), # 64 * 64 * 64\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv8 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    \"\"\"self.Conv8 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(64, 32, 2, output_padding = 64), # 128 * 128 * 32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\"\"\"\n",
        "    \n",
        "    self.Conv9 = nn.Sequential(\n",
        "    nn.Conv2d(32, 2, 2, padding = 'same') # 128 * 128 * 2\n",
        "    )\n",
        "\n",
        "    \n",
        "    self.instance_upscaled = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=4, padding=1, output_padding=1) # from 8x8 to 32x32\n",
        "\n",
        "\n",
        "  def forward(self, x, embed):\n",
        "    x = self.Conv0(x)\n",
        "    x = self.Conv1(x)\n",
        "    x = self.Conv2(x)\n",
        "    x = self.Conv3(x)\n",
        "    x = self.Conv4(x)\n",
        "    embed = self.instance_upscaled(embed)\n",
        "    #print (x.size(), embed.size())\n",
        "    fuse = torch.cat((x, embed),1)\n",
        "    x = self.fused(fuse)\n",
        "    x = self.Conv5(x)\n",
        "    x = self.Conv6(x)\n",
        "    x = self.Conv7(x)\n",
        "    x = self.Conv8(x)\n",
        "    x = self.Conv9(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "vDzFiMq7WF_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#our model with instance without unet res connections\n",
        "class colorizer_transpose(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.name = \"colorizer_transpose\"\n",
        "    super(colorizer_transpose, self).__init__()\n",
        "    self.Conv0 = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding = 'same'), # 128 * 128 * 32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.Conv1 = nn.Sequential(\n",
        "    nn.Conv2d(32, 64, 3, stride = 2, padding = 1), # 64*64*64\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv2 = nn.Sequential(\n",
        "    nn.Conv2d(64, 128, 2, padding = 'same'), # 64 * 64 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.Conv3 = nn.Sequential(\n",
        "    nn.Conv2d(128, 256, 2, stride = 2, padding = 0), # 32 * 32 * 256\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv4 = nn.Sequential(\n",
        "    nn.Conv2d(256, 512, 2,stride = 2, padding = 0), # 32 * 32 * 512\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.fused = nn.Sequential(\n",
        "    nn.Conv2d(1024, 512, 2, padding = 'same'), # 32 * 32 * 512\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv5 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv6 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    \"\"\"self.Conv6 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(256, 128, 2, output_padding = 32), # 64 * 64 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\"\"\"\n",
        "    self.Conv7 = nn.Sequential(\n",
        "    nn.Conv2d(128, 64, 2, padding = 'same'), # 64 * 64 * 64\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv8 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    \"\"\"self.Conv8 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(64, 32, 2, output_padding = 64), # 128 * 128 * 32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\"\"\"\n",
        "    \n",
        "    self.Conv9 = nn.Sequential(\n",
        "    nn.Conv2d(32, 2, 2, padding = 'same') # 128 * 128 * 2\n",
        "    )\n",
        "\n",
        "    \n",
        "    self.instance_upscaled = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=4, padding=1, output_padding=1) # from 8x8 to 32x32\n",
        "\n",
        "\n",
        "  def forward(self, x, embed):\n",
        "    x = self.Conv0(x)\n",
        "    x = self.Conv1(x)\n",
        "    x = self.Conv2(x)\n",
        "    x = self.Conv3(x)\n",
        "    x = self.Conv4(x)\n",
        "    #embed = self.instance_upscaled(embed)\n",
        "    #print (x.size(), embed.size())\n",
        "    #fuse = torch.cat((x, embed),1)\n",
        "    #x = self.fused(fuse)\n",
        "    x = self.Conv5(x)\n",
        "    x = self.Conv6(x)\n",
        "    x = self.Conv7(x)\n",
        "    x = self.Conv8(x)\n",
        "    x = self.Conv9(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "yY8DUGH_KSer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the embedding upsampling layer\n",
        "class upscaler(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.name = \"upscaler\"\n",
        "    super(upscaler, self).__init__()\n",
        "    self.instance_upscaled = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=4, padding=1, output_padding=1) # from 8x8 to 32x32\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.instance_upscaled(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ucbvq4hSaS7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = upscaler()\n",
        "model.to('cuda')\n",
        "\n",
        "summary(model, input_size=(1, 1024, 8, 8))"
      ],
      "metadata": {
        "id": "bKyJ9RwrCVG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381cdb55-b0fe-4ed1-cc7f-d031af94afea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "upscaler                                 --                        --\n",
              "├─ConvTranspose2d: 1-1                   [1, 512, 32, 32]          13,107,712\n",
              "==========================================================================================\n",
              "Total params: 13,107,712\n",
              "Trainable params: 13,107,712\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 13.42\n",
              "==========================================================================================\n",
              "Input size (MB): 0.26\n",
              "Forward/backward pass size (MB): 4.19\n",
              "Params size (MB): 52.43\n",
              "Estimated Total Size (MB): 56.89\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XdIP8p0eMuV"
      },
      "outputs": [],
      "source": [
        "from timm.models.layers.activations import tanh\n",
        "# OUR MODEL\n",
        "class colorizer_UNET(nn.Module):           \n",
        "  def __init__(self):\n",
        "    self.name = \"colorizer_UNET\"\n",
        "    super(colorizer_UNET, self).__init__()\n",
        "    self.instance_upscale = nn.Sequential(\n",
        "    nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=4, padding=1, output_padding=1), # from 8x8 to 32x32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv0 = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding = 'same'), # 128 * 128 * 32 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.Conv1 = nn.Sequential(\n",
        "    nn.Conv2d(32, 64, 3, stride = 2, padding = 1), # 64 * 64 * 64 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv2 = nn.Sequential(\n",
        "    nn.Conv2d(64, 128, 2, padding = 'same'), # 64 * 64 * 128 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.Conv3 = nn.Sequential(\n",
        "    nn.Conv2d(128, 256, 2, stride = 2, padding = 0), # 32 * 32 * 256 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv4 = nn.Sequential(\n",
        "    nn.Conv2d(256, 512, 2, padding = 'same'), # 32 * 32 * 512 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv5 = nn.Sequential(\n",
        "    nn.Conv2d(512, 512, 2, stride = 2, padding = 0), # 16 * 16 * 512 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.fusion = nn.Sequential(\n",
        "    nn.Conv2d(1024, 512, 2, padding = 'same'), # 16 * 16 * 512+512 (embed + conv5) input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.postfusion =nn.Sequential(\n",
        "    nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1), # 32 * 32 output, 256+256 input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv6 = nn.Sequential(\n",
        "    nn.Conv2d(1024, 256, 2, padding = 'same'), # 16 * 16 * 512+512 (fusion + conv4) input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    # self.Conv7 = nn.Sequential(\n",
        "    # nn.Upsample(scale_factor=2),\n",
        "    # nn.Conv2d(512, 128, 2, padding = 'same'), # 64 * 64 * 128\n",
        "    # nn.ReLU(),\n",
        "    # nn.BatchNorm2d(128)\n",
        "    # )\n",
        "    self.Conv7 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1), # 32 * 32 output, 256+256 input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.Conv8 = nn.Sequential(\n",
        "    nn.Conv2d(256, 64, 2, padding = \"same\"), # 64 * 64 * 128 \n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv9= nn.Sequential(\n",
        "    nn.ConvTranspose2d(128, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # 128 * 128 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    ) \n",
        "    self.Conv10 = nn.Sequential(\n",
        "    nn.Conv2d(64, 2, 2, padding = 'same')#, # 128 * 128 * 2\n",
        "    #nn.Tanh()\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "  def forward(self, x, instance_embed):\n",
        "    encode1 = self.Conv0(x)\n",
        "    encode2 = self.Conv1(encode1)\n",
        "    encode3 = self.Conv2(encode2)\n",
        "    encode4 = self.Conv3(encode3)\n",
        "    encode5 = self.Conv4(encode4)\n",
        "    bottleneck = self.Conv5(encode5)\n",
        "    instance_embed = self.instance_upscale(instance_embed)\n",
        "    fused = torch.cat((instance_embed,bottleneck),1)\n",
        "    fusion = self.fusion(fused) \n",
        "    fusion = self.postfusion(fusion)\n",
        "    fused = torch.cat((fusion, encode5), 1) \n",
        "    decode1 = self.Conv6(fused)\n",
        "    decode1 = torch.cat((decode1, encode4), 1)\n",
        "    decode2 = self.Conv7(decode1)\n",
        "    decode2 = torch.cat((decode2, encode3), 1)\n",
        "    decode3 = self.Conv8(decode2)\n",
        "    #print(decode3.size(),encode2.size())\n",
        "\n",
        "    decode3 = torch.cat((decode3, encode2), 1)\n",
        "    decode4 = self.Conv9(decode3)\n",
        "    decode4 = torch.cat((decode4, encode1), 1)\n",
        "    decode5 = self.Conv10(decode4)\n",
        "    return decode5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Helper functions"
      ],
      "metadata": {
        "id": "sPwgmfYVHdc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "akMJbGApIqoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Alternate Loss Functions ##################\n",
        "''' SSIM:\n",
        "Source: https://stackoverflow.com/questions/53956932/use-pytorch-ssim-loss-function-in-my-model\n",
        "'''\n",
        "##### SSIM LOSS ############\n",
        "class SSIMLoss(piqa.ssim.SSIM):\n",
        "    def forward(self, x, y):\n",
        "        return 1. - super().forward(x, y)\n",
        "\n",
        "class PSNRLoss(piqa.psnr.PSNR):\n",
        "    def forward(self, x, y):\n",
        "        return piqa.psnr.psnr(x, y, value_range=128).mean()"
      ],
      "metadata": {
        "id": "IaEsm0_Wv-_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBD2VyCUCWbA"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, val_loader, embed, criterion, model_path, epoch, save_images):   \n",
        "  model.eval()\n",
        "  losses = AverageMeter()\n",
        "  with torch.no_grad():\n",
        "    for i, (input_gray, input_ab) in enumerate(val_loader):\n",
        "      input_gray, input_ab = input_gray.cuda(), input_ab.cuda()\n",
        "\n",
        "      output_ab = model(input_gray,embed)\n",
        "      loss = criterion (output_ab,input_ab)\n",
        "      losses.update(loss.item(), input_gray.size(0))\n",
        "      if save_images:\n",
        "        for j in range(min(len(output_ab), 5)): # save at most 5 images\n",
        "            save_path = {'grayscale': str(model_path) + 'outputs/gray/', 'colorized': str(model_path)+'outputs/color/'}\n",
        "            save_name = 'img-{}-epoch-{}.jpg'.format(j, epoch)\n",
        "            to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
        "  return losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy1aeAa-y_X-"
      },
      "outputs": [],
      "source": [
        "def split_data(data_path, batch_size=1) :\n",
        "  # Training\n",
        "  train_transforms = transforms.Compose([])#transforms.Compose([transforms.Resize(128)])\n",
        "  train_imagefolder = MockData(data_path + '/train', train_transforms)\n",
        "  train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # Validation \n",
        "  val_transforms = transforms.Compose([])\n",
        "  val_imagefolder = MockData(data_path + '/val' , val_transforms)\n",
        "  val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDF60mzggHCI"
      },
      "outputs": [],
      "source": [
        "## MOCK Dataloader ##\n",
        "## Resources: \n",
        "# https://towardsdatascience.com/dataloader-for-sequential-data-using-pytorch-deep-learning-framework-part-2-ed3ad5f6ad82\n",
        "# https://medium.com/analytics-vidhya/writing-a-custom-dataloader-for-a-simple-neural-network-in-pytorch-a310bea680af\n",
        "# https://blog.paperspace.com/dataloaders-abstractions-pytorch/\n",
        "class MockData(torch.utils.data.Dataset):   \n",
        "   def __init__(self, path, transforms):\n",
        "      super(torch.utils.data.Dataset, self).__init__()\n",
        "      self.input_files = os.listdir(path) # e.g. content/small_dataset/train\n",
        "      self.transform = transforms\n",
        "      self.path = path\n",
        "   def __len__(self):\n",
        "        return len(self.input_files)\n",
        "   def __getitem__(self, index):\n",
        "       input_filename = self.input_files[index]\n",
        "       ''' Code before we saved lab images\n",
        "       print(input_filename)\n",
        "       image = Image.open(os.path.join(self.path, input_filename))\n",
        "\n",
        "       image = transforms.ToTensor()(image)\n",
        "       #print(self.input_files[index], image.shape)\n",
        "\n",
        "       image =  transforms.Resize((128, 128))(image)\n",
        "       image = torch.swapaxes( torch.swapaxes(image, 0, 2), 0, 1)\n",
        "       image = color.rgb2lab(1.0/255*image)\n",
        "       image = torch.swapaxes(torch.from_numpy(image), 0, 2)\n",
        "\n",
        "       image_bw = image[0,:,:]\n",
        "       image_bw = torch.unsqueeze( image_bw, 0)\n",
        "       label = image[1:,:,:]\n",
        "       label /= 128\n",
        "\n",
        "       #print(image_bw)\n",
        "       #print(label)\n",
        "       '''\n",
        "\n",
        "       image = np.load(os.path.join(self.path, input_filename))\n",
        "\n",
        "       image_bw = image[:,:,0]\n",
        "       label = image[:,:,1:]\n",
        "\n",
        "       image_bw = image_bw.reshape(1, 128, 128)\n",
        "       label = np.swapaxes( np.swapaxes(label, 0, 2 ), 2, 1)\n",
        "\n",
        "       image_bw = torch.from_numpy(image_bw).float()\n",
        "       label = torch.from_numpy(label).float()\n",
        "\n",
        "       return image_bw, label "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model and training"
      ],
      "metadata": {
        "id": "xQIX38-yImAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cspdarknet53 = timm.create_model('cspdarknet53', pretrained=True, features_only = True, in_chans=1)\n",
        "#cspdarknet53.to('cuda')\n",
        "\n",
        "summary(cspdarknet53, input_size=(1,1,128,128))"
      ],
      "metadata": {
        "id": "C8KQztSYzrQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OUR MODEL v2\n",
        "class colorizer_UNETv2(nn.Module):           \n",
        "  def __init__(self):\n",
        "    self.name = \"colorizer_UNETv2\"\n",
        "    super(colorizer_UNETv2, self).__init__()\n",
        "    self.instance_upscale = nn.Sequential(\n",
        "    nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=4, padding=1, output_padding=1), # from 8x8 to 32x32\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.Conv0 = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding = 'same'), # 128 * 128 * 32 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.Conv1 = nn.Sequential(\n",
        "    nn.Conv2d(32, 64, 3, stride = 2, padding = 1), # 64 * 64 * 64 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.Conv2 = nn.Sequential(\n",
        "    nn.Conv2d(64, 128, 2, padding = 'same'), # 64 * 64 * 128 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.Conv3 = nn.Sequential(\n",
        "    nn.Conv2d(128, 256, 2, stride = 2, padding = 0), # 32 * 32 * 256 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.Conv4 = nn.Sequential(\n",
        "    nn.Conv2d(256, 512, 2, padding = 'same'), # 32 * 32 * 512 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.bottleneck1 = nn.Sequential(\n",
        "    nn.Conv2d(512, 512, 2, stride = 2, padding = 0), # 16 * 16 * 512 output\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.fusion = nn.Sequential(\n",
        "    nn.Conv2d(1024, 512, 2, padding = 'same'), # 16 * 16 * 512+512 (embed + conv5) input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.bottleneck2 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1), # 32 * 32 output, 512+512 input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.decode4 =nn.Sequential(\n",
        "    nn.Conv2d(1024, 256, 2, padding = 'same'), # 32 * 32 * 512 input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.decode3 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1), # 32 * 32 output, 256+256 input\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.decode2 = nn.Sequential(\n",
        "    nn.Conv2d(256, 64, 2, padding = \"same\"), # 64 * 64 * 128 \n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.decode1= nn.Sequential(\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # 128 * 128 * 128\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32)\n",
        "    ) \n",
        "    self.decode0 = nn.Sequential(\n",
        "    nn.Conv2d(64, 2, 2, padding = 'same')#, # 128 * 128 * 2\n",
        "    #nn.Tanh()\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "  def forward(self, x, instance_embed):\n",
        "    encode1 = self.Conv0(x)\n",
        "    encode2 = self.Conv1(encode1)\n",
        "    encode3 = self.Conv2(encode2)\n",
        "    encode4 = self.Conv3(encode3)\n",
        "    encode5 = self.Conv4(encode4)\n",
        "    bottleneck = self.bottleneck1(encode5)\n",
        "    instance_embed = self.instance_upscale(instance_embed)\n",
        "    fused1 = torch.cat((instance_embed,bottleneck),1)\n",
        "    fusion = self.fusion(fused1) \n",
        "    bottleneck2 = self.bottleneck2(fusion)\n",
        "    fused2 = torch.cat((bottleneck2, encode5), 1) \n",
        "    decode4 = self.decode4(fused2)\n",
        "    decode3 = self.decode3(decode4)\n",
        "    fused3 = torch.cat((decode3, encode3), 1)\n",
        "    decode2 = self.decode2(fused3)\n",
        "    decode1 = self.decode1(decode2)\n",
        "    fused4 = torch.cat((decode1, encode1), 1)\n",
        "    decode0 = self.decode0(fused4)\n",
        "    return decode0\n"
      ],
      "metadata": {
        "id": "G39bYdTPo3_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-PG0ZMy7OsQ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset_path, learning_rate=0.01, batch_size=64, num_epochs=1):\n",
        "    model = model.to(device)\n",
        "    train_loader, val_loader = split_data(dataset_path, batch_size)\n",
        "    criterion = PSNRLoss().to(device) # nn.MSELoss().to(device) # Alternatives, PSNR, PA (https://core.ac.uk/download/pdf/151072499.pdf) \n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose= True, min_lr=1e-6)\n",
        "\n",
        "    iters, train_losses, val_losses = [], [], []\n",
        "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.7, verbose= True, min_lr=1e-6)\n",
        "    model_name = get_model_name(batch_size, learning_rate, num_epochs) \n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/APS360_final_training/backup\", model_name)\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    start_time = time.time() \n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      for i, (bw_images,labels) in enumerate(train_loader):\n",
        "        #bw_rgb = greyLAB2RGB(bw_images, batch_size).to(device)\n",
        "        bw_images = bw_images.cuda()\n",
        "        # print(bw_rgb.size())\n",
        "        #bw_images = bw_images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "        embed = cspdarknet53(bw_images)[-1].to(device)\n",
        "        \n",
        "        #print(bw_images.shape, embed.size())\n",
        "        logits = model(bw_images,embed)\n",
        "\n",
        "        #to_rgb(bw_images.cpu()[0], labels.cpu()[0])\n",
        "        #print(\"logits shape = \" + str(logits.shape)) >> [batch_size, 2, 128, 128]\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      # Display a few images before and after for each epoch\n",
        "      if epoch % 2 == 1:\n",
        "        #print(\"hello\")\n",
        "        iters.append(epoch)\n",
        "        train_losses.append(float(loss))  \n",
        "        val_losses.append(float(get_accuracy(model, val_loader, embed, criterion, model_path, epoch, False)))  # compute validation accuracy\n",
        "        print(\"Epoch [{}/{}], Loss: {:.4f}, Training losses: {:.4f}, Validation losses: {:.4f}\".format(epoch+1,num_epochs,loss,train_losses[-1],val_losses[-1]))\n",
        "        scheduler.step(val_losses[-1])\n",
        "        try: \n",
        "          if val_losses[-1]==min(val_losses): #model saved if performance is better \n",
        "            torch.save(model.state_dict(), model_path + \"/\" + \"best\")\n",
        "            print(\"best model saved at epoch {}\".format(epoch+1))\n",
        "        except:\n",
        "          continue\n",
        "      if epoch%10 == 0: #model checkpoint every 20 epochs\n",
        "        torch.save(model.state_dict(), model_path + \"/\" + get_model_name(batch_size, learning_rate, epoch))\n",
        "        print (\"checkpoint saved at epoch {}\".format(epoch+1)) \n",
        "        quick_qualitative(model, \"/content/large_dataset\", 10)\n",
        "    \n",
        "    print(\"Total time taken: {}\".format(time.time() - start_time))   \n",
        "  \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_losses, label=\"Train\")\n",
        "    plt.plot(iters, val_losses, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.savefig(str(model_path) + '/training curve.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING\n",
        "low_lr = colorizer_UNETv2()\n",
        "low_lr.load_state_dict(torch.load(\"/content/land_miracle\"))\n",
        "train(low_lr, '/content/large_dataset', learning_rate=0.0005, batch_size=16, num_epochs=51)\n",
        "# torch.save(model.state_dict(), \"/content/drive/MyDrive/APS360_final_training/backup/bs16_lr0.01_epoch51/bs16_lr0.01_final\")"
      ],
      "metadata": {
        "id": "yY_PiJJmJA2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing\n"
      ],
      "metadata": {
        "id": "dCP3kRN8Y15O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Val Qualitative Results\n",
        "train_loader, val_loader = split_data('/content/large_dataset', 1)\n",
        "criterion = PSNRLoss().to(device)\n",
        "total_loss = []\n",
        "\n",
        "for inputs, labels in enumerate(val_loader): #change the loader here for train\n",
        "  bw_rgb = greyLAB2RGB(inputs,1).cuda()\n",
        "  inputs = inputs.cuda()\n",
        "  print(inputs.size())\n",
        "  labels = labels.cuda()\n",
        "  embed = cspdarknet53(bw_rgb)[-1].to(device)\n",
        "\n",
        "  logits = model(inputs,embed)\n",
        "  loss = criterion(logits, labels)\n",
        "  total_loss.append(loss.item())\n",
        "\n",
        "  print(\"Attempt at Reconstruction\")\n",
        "  to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "  print(\"Original\")\n",
        "  to_rgb(inputs.cpu()[0], labels.cpu()[0])\n",
        "print (f\"psnr is {sum(total_loss)/len(total_loss)}\")"
      ],
      "metadata": {
        "id": "E2AivAxGm8XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8xyG11z-Tih"
      },
      "outputs": [],
      "source": [
        "#Test results\n",
        "test_transforms = transforms.Compose([])\n",
        "test_imagefolder = MockData(\"/content/large_dataset\" + '/test' , test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(test_imagefolder, batch_size=1, shuffle=False)\n",
        "best = colorizer_UNETv2().to(device)\n",
        "best.load_state_dict(torch.load(\"/content/land_miracle\", map_location=torch.device('cpu')))\n",
        "best.eval()\n",
        "criterion = PSNRLoss().to(device) # nn.MSELoss().to(device) # Alternatives, PSNR, PA (https://core.ac.uk/download/pdf/151072499.pdf) \n",
        "total_loss = []\n",
        "numb_t = 0\n",
        "for inputs, labels in test_loader:\n",
        "  # bw_rgb = greyLAB2RGB(inputs,1).cuda()\n",
        "\n",
        "  #inputs = inputs\n",
        "  #labels = labels\n",
        "  embed = cspdarknet53(inputs)[-1].to(device)\n",
        "\n",
        "  logits = best(inputs,embed)\n",
        "\n",
        "  loss = criterion(logits, labels)\n",
        "  #print(loss.item())\n",
        "  total_loss.append(loss.item())\n",
        "  # print(\"Attempt at Reconstruction\")\n",
        "  # to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "  # print(\"Original\")\n",
        "  # to_rgb(inputs.cpu()[0], labels.cpu()[0])\n",
        "  numb_t += 1\n",
        "  if numb_t%10 == 0:\n",
        "  \n",
        "    print(\"Demonstrating Some Test Set Results\")\n",
        "    print(\"Attempt at Reconstruction\")\n",
        "    to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "    print(\"Original\")\n",
        "    to_rgb(inputs.cpu()[0], labels.cpu()[0])\n",
        "    print(loss.item())\n",
        "print (f\"psnr is {sum(total_loss)/len(total_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on custom image"
      ],
      "metadata": {
        "id": "gneHVoiDG-pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #preprocessing image\n",
        "  small_size = 128\n",
        "  rgb = Image.open(\"/content/lina-loos-04-C1NZk1hE-unsplash.jpg\")\n",
        "  old_size = rgb.size  # old_size is in (width, height) format\n",
        "  desired_size = max(old_size)\n",
        "  delta_w = desired_size - old_size[0]\n",
        "  delta_h = desired_size - old_size[1]\n",
        "  padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "  new_im = ImageOps.expand(rgb, padding) # padding done\n",
        "  new_im = new_im.resize((small_size,small_size)) # resizing done\n",
        "  # https://stackoverflow.com/questions/3228361/using-pil-and-numpy-to-convert-an-image-to-lab-array-modify-the-values-and-then\n",
        "  npArray = np.asarray(new_im)\n",
        "  labArray = color.rgb2lab(npArray) # Convert array from RGB into Lab\n",
        "  np.save(\"/content/0000000\",labArray) # saving"
      ],
      "metadata": {
        "id": "1lFRvmWNakBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image loader\n",
        "image = np.load(\"/content/0000000.npy\")\n",
        "\n",
        "image_bw = image[:,:,0]\n",
        "label = image[:,:,1:]\n",
        "\n",
        "image_bw = image_bw.reshape(1, 128, 128)\n",
        "label = np.swapaxes(np.swapaxes(label, 0, 2 ), 2, 1)\n",
        "\n",
        "#model\n",
        "model = colorizer_UNETv2().to(device)\n",
        "model.load_state_dict(torch.load(\"/content/land_miracle\", map_location=torch.device('cpu')))\n",
        "inputs = torch.from_numpy(image_bw).float()\n",
        "inputs = torch.unsqueeze(inputs,0)\n",
        "print(inputs.size())\n",
        "labels = torch.from_numpy(label).float()\n",
        "\n",
        "model.eval()\n",
        "embed = cspdarknet53(inputs)[-1]\n",
        "\n",
        "logits = model(inputs,embed)\n",
        "print(\"Attempt at Reconstruction\")\n",
        "to_rgb(inputs.cpu()[0], logits.cpu().detach().numpy()[0])\n",
        "print(\"Original\")\n",
        "to_rgb(inputs.cpu()[0], labels.cpu()[0])"
      ],
      "metadata": {
        "id": "caO8Vsyeeyqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}